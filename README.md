# Большие данные

# каталог

- [Функции, реализованные в этой системе] (#функции, реализованные в этой системе)
- [Разговор об опыте] (#разговор об опыте)
- [Как сервер нужен] (#как сервер нужен)
- [Рекомендуемый способ развертывания hdfs] (#рекомендуемый способ развертывания hdfs)
- [hdfs_nameservice деление] (#hdfs_nameservice деление)
- [спецификация каталога hdfs] (#спецификация каталога hdfs)
- [zookeeper Cluster Division] (#zookeeper Cluster Division)
- [yarn Cluster Division] (#yarn Cluster Division)
- [спецификация yarn_queue] (спецификация # yarn_queue)
- [kafka Cluster Division] (#kafka Cluster Division)
- [спецификация Кафки] (#спецификация Кафки)
- [Разделение кластера коллекционных услуг Canal/Flume] (#разделение кластера коллекционных услуг)
- [разделение кластера hbase] (#разделение кластера hbase)
- [спецификация data_warehouse] (спецификация # data_warehouse)
- [Спецификация журнала ведения бизнеса] (#спецификация журнала ведения бизнеса)
- [Принципы рекомендаций по разделению кластеров] (#принципы рекомендаций по разделению кластеров)
- [планирование главного узла] (#планирование главного узла)
- [Системный раздел и монтирование] (#системный раздел и монтирование)
- [Где метаданные] (#где метаданные)
- [Хороший патч] (#хороший патч)
- [Слияние небольших файлов] (#слияние небольших файлов)
- [Сигнал тревоги монитора] (#сигнал тревоги монитора)
- [Сетевая плоскость] (#сетевая плоскость)
- [Сеть кластеров] (#сеть кластеров)
- [Конфигурация сетевой карты хоста] (#конфигурация сетевой карты хоста)
- [Типичная настройка бизнеса] (#типичная настройка бизнеса)
- [Предпосылки развертывания установки компонентов] (#предпосылки развертывания установки компонентов)
- [Порядок установки компонентов] (#порядок установки компонентов)
- [Шаги установки] (#шаги установки)
- [Автоматическое развертывание и расширение компонентов больших данных] (#автоматическое развертывание и расширение компонентов больших данных)
- [Шаги по управлению и расширению журнала компонентов] (#шаги по управлению и расширению журнала компонентов)
- [расширение hdfs] (#расширение hdfs)
- [NameNode/ldap/kdc / keytab Backup & Recovery] (#namenode/ldap/kdc/keytab Backup & Recovery)
- [namenode-standy/Active Doing] (#namenode-standy / active Doing)
- [Принцип ranger компонента безопасности/Примечание/плагин включен] (#принцип ranger компонента безопасности/Примечание/плагин включен)
- [kerberos on / jce authentication keytab отсутствует как обрабатывать] (#kerberos on / jce authentication keytab отсутствует как обрабатывать)
- [Обновление компонентов] (#обновление компонентов)
- [Компонентная сигнализация монитора] (#компонентная сигнализация монитора)
- [Каталог сервера загрузки] (#каталог сервера загрузки)
- [шаги разработки ambari_service] (шаги разработки # ambari_service)
- [Big Data Reference] (#Big Data Reference)


### Функции, реализованные в этой системе
``
Реализует общие функции
Основные из них:
 1 Поддержка компонента "большие данные" (tar.пакет gz) автоматическая установка обновления развертывания
 Конфигурация 2 компонентов-это проверенная производственная практика (100PB + Data, 2k + server, 20w+job / Day)
 3 Установка 68 компонентов обновление развертывания
 4 мониторинг выживаемости компонентов и мониторинг url
 5 централизованное управление журналами компонентов
 6 операционный аудит (hdfs, hive,hbase,kafka, storm и т. д.), Чтобы знать, кто и когда работал с какими ресурсами
 7 контроль разрешений (поддержка hdfs,hive,hbase,kafka,storm и т. д.) поддержка управления правами на уровне рангов
 8 мониторинг сигналов тревоги (не только мониторинг системы, но и мониторинг различных компонентов JVM metrics)
 9 автоматическое расширение емкости
``

### Разговор об опыте
``
 1, бизнес только начинается, рекомендуется использовать Google analytics (поддержка пользовательских событий и аналитика событий)/friends / Tencent Analytics, не достаточно, чтобы рекомендовать использование большого пакета данных Ali Cloud или приватизированное развертывание данных, наконец, не слишком хорошо, чтобы построить себя по той простой причине, что большие данные-это длинный канал, чтобы инвестировать в большие проблемы, третьи стороны предоставляют не только подготовку среды, но и набор методов и спецификаций
 2, построить свой собственный случай, чтобы иметь план стойки в течение следующих 3 лет или около того(если бизнес быстро развивается без планирования, что приводит к деньгам, чтобы купить машину, но не может купить положение стойки, то вы можете только переместить машинное отделение, а перемещение машинное помещение для копирования данных занимает несколько месяцев.)
 3, независимо от того, сколько кластеров имеет только один набор ldap (управление пользователями,может быть мастер из синхронизации) один набор kdc (Security ticket) один набор ranger (авторизация и аудит) один набор atlas (управление метаданными)
 4, раннее создание, включая спецификацию каталога hdfs, спецификацию очереди yarn,спецификацию таблицы подсчета, спецификацию номенклатуры kafka topic
 5, создание механизма изоляции нескольких арендаторов как можно раньше, механизм аудита журнала (решается с помощью ranger)
 6, создание механизма quota как можно раньше, функция периодического слияния небольших файлов hdfs
 7, как можно скорее установить спецификацию места захоронения и систему места захоронения, спецификацию журнала игры дела
 8, создание системы метаданных и системы родства данных как можно раньше(доступно с открытым исходным кодом atlas）
 9, можно через Кафку как канал передачи данных с Кафкой схемы регистрации управлять всей схемой связи
``

### Рекомендуемый способ развертывания hdfs
 
 ![hdfs] (img / hdfs.png) 


## раздел # hdfs_nameservice
``
 Разделение нескольких кластеров для совместного использования общих данных через федерацию
 1 Ярн логс, ТМП 
 2 складских помещения
 3 hbase
 4 поток
``

## спецификация каталога # hdfs
``
 Разделение каталогов по бизнесу или группе
 Ниже приведены рекомендуемые корневые каталоги, которые затем делятся на бизнес или группы на основе этого корневого каталога
 /журнал журнала Ярн / журнал Искры / журнал г-на 
 / склад номер склада
 временный каталог / tmp
 / каталог hbase hbase
 каталог пользователя / user
 / revenue доход
 / ext расширение для хранения банок и т. д
 / контрольные точки 
``

## разделение кластера # zookeeper
``
 Разделение нескольких кластеров повышает стабильность
 1 хбасе / поток / Кафка
 2 склад / пакет
 3 другие
``

## разделение кластера # yarn
``
 Разделение на несколько кластеров повышает стабильность, а затем может быть унифицировано с распределением ресурсов через yarn federation
 1, поток
 2, batch
 3, другое
``

## спецификация # yarn_queue
``
 По отделам.Деление очередей на бизнес-группы
 Как департмент.бизнес
``

### Кафка кластерное разделение
``
 Разделение нескольких кластеров повышает стабильность
 1, отчет о поведении пользователя / nginx logs
 2, БД binlog
 3, другое
``

### спецификация Кафки
``
 Спецификация номенклатуры темы в Кафке: department_business_feature
 kafka работает лучше всего, когда количество разделов в 4 раза больше, чем количество дисков (т. е. 4 раздела на диск
 число рабочих предпочтительно равно числу партионов

``

### Разделение кластера коллекторских услуг
``
 В автономном режиме
 в реальном времени
``

### разделение кластера hbase
``
 Отдельные кластеры для крупных предприятий
 Публичный / другой бизнес изолирован через группу regionserver
``

## спецификация # data_warehouse
``
 Важно, чтобы количество складов было многоуровневым и управление качеством данных
 Прямым результатом плохой стратификации данных является пустая трата вычислительных ресурсов и ресурсов хранения
 Качество, но данные не так хороши, как нет. 
 Уровни ODS и DWD стараются быть мультиплексированными с автономными ссылками
 
 https://blog.csdn.net/zhaodedong/article/details/85293955
 https://blog.csdn.net/zhaodedong/article/details/73385647
 http://tech.weli.cn/2017/12/29/dataware-intro/
 
 Ссылка на спецификацию именования подсчета Али:
 правила именования уровня ods: ods_{тип исходной системы} _ {имя исходной системной таблицы} _ [{идентификатор цикла обновления}{инкрементный полный идентификатор}] Unified Device table-realtime (DataHub): ods_dcs_prd_mc_point_r Unified Device table-offline (MaxCompute): ods_dcs_prd_mc_point_di
 правило именования слоя dws: dws_{первичный предметный домен} _ {вторичный предметный домен} _ {гранулярность данных} _ {бизнес-имя} _ {статистический период} пример: конвертерный ковш гранулярность за последние 1 день сводная таблица dws_prd_sm_ladle_bof_1d Примечание: при необходимости бизнес-имя может быть разбито на несколько комбинаций идентификаторы цикла обновления и инкрементные полные идентификаторы не являются обязательными на уровне dws, по умолчанию не нужно добавлять
 правило именования dwd-уровня: dwd_{предметная область первого уровня}_{предметная область второго уровня}_{бизнес-имя}_{идентификатор цикла обновления}{инкрементный полный идентификатор} пример: Дельта-таблица подробностей конвертерного сталеплавильного производства dwd_prd_sm_bof_hi Примечание: при необходимости бизнес-имя может быть разбито на несколько комбинаций

``
![datawarehouse] (img / dw.джпг)
![datawarehouse0] (img / dw0.джпг)
![datawarehouse1] (img / dw1.джпг)
![datawarehouse2] (img / dw2.джпг)
![datawarehouse3] (img / dw3.джпг)
![datawarehouse4] (img / dw4.джпг)
![datawarehouse5](img/dw5.джпг)
![datawarehouse6] (img / dw6.джпг)
![datawarehouse7] (img / dw7.джпг)
![datawarehouse8] (img / dw8.джпг)

### Какой сервер нужен
``
 Принцип развертывания компонентов больших данных заключается в том, что гибридные вычисления находятся как можно ближе к хранилищу, и большинство операций append не изменяются после записи
 В основном нужны 3 типа машин
 Класс 1 является мастером (около 10 единиц достаточно)
 Типичный, например, namenode требует большой памяти (рекомендуется 512G) 4 SSD-диска для резервного копирования raid1
 Тип 1 тип машина хранения, разделенный в 2 вида одной главной машины одна модель архива
 Основные модели (например, Wave NF5280M5, nf5466m5 / Huawei 2298V5,5288V5) типичные машины, такие как datanode / kafka 2U, 12 блоков спереди+4 диска сзади, каждый диск 8T, встроенный 2 диска ssd, SATA-диск не делает RAID-память также пытается 256G выше
 Архивные модели (например, Wave NF5486M5/Huawei 5288V5) машина 4U поддерживает 106 дисков 8T
 Класс 1-это клиентская машина
 Роль шлюза, которая обычно используется для отправки задания, как правило, виртуальная машина
 Другие критически важные бизнес-серверы (hot/warn/cold в hot Models)
 (Например НФ5180М5,НФ8260М5 / хуавай 2288В5,2488В5)
 
 Интернет
 Одна 10-гигабитная сетевая карта может быть без Бонда 
 kafka-это типичное сетевое узкое место, которое можно настроить с помощью двух гигабитных сетевых карт, чтобы сделать bond6
 
``

### Системный раздел и монтирование
``
 Основной момент заключается в том, что/var должен быть разделен на 100G / 200G в основном используется для хранения журналов больших данных очень много журналов, хотя он будет автоматически вращаться, но также боится записи / каталогов
 
 master / var / log / data0 / data1
 данные / var /log /data0 /data1 /data2 /data3 /data4 /data5 /data6 /data7 /data8 / data9 / data10 / data11
 клиент / var / log
 
``

## Где # метаданные
``
 Большие компоненты данных главным образом в БД метасервер улей, ЗК, БД, хдфс, Кафка , хбасе 
 
 пользовательские данные linux в основном находятся в билетах безопасности ldap в kdc
``


### Бизнес играет спецификацию журнала
``
 http://blog.jobbole.com/56574/
 https://zhuanlan.zhihu.com/p/27363484
 https://dbaplus.cn/news-134-1658-1.html
``
 
### Хороший патч
``
 https://jira.apache.org/jira/browse/YARN-4948
 https://issues.apache.org/jira/browse/HDFS-2139
 https://issues.apache.org/jira/browse/HDFS-12943
 https://issues.apache.org/jira/browse/HDFS-8966
 https://issues.apache.org/jira/browse/SPARK-23128
``
### Слияние небольших файлов
``
паркет: hadoop jar ./ паркет-инструменты-1.11.0.jar merge-b-l 512 / input_directory / / output_idr / имя_файла
 
 После слияния удаляются исходные файлы для небольших файлов в/input_directory/ (в выходной структуре приведенной выше команды есть список Объединенных файлов）

``
### Контрольная сигнализация
``
 2 способа:
 1, в grafana config Alarm, grafana версии>5 поддерживаются(рекомендуется)
 2, реализация alert_dispatcher.пользовательская функция сигнализации в py .Затем выполните alert_dispatcher.sh добавьте эту функцию оповещения к серверу
``
### Рекомендации по разделению кластеров
``
 Объединение данных(DataNode) (с разделением namenode через федерацию hdfs), разделение вычислений (через yarn nodelabel или yarn federation）
 Изолировать рабочие нагрузки в реальном времени и не в реальном времени: используйте отдельные кластеры для нагрузок, требующих более высокой производительности в реальном времени.
 В зависимости от фундаментальных различий между ресурсами оборудования (включая CPU/Memory/Disk/Network), рекомендуется разделить развертывание различных служб на уровни соглашения об уровне обслуживания.Такие приложения, как Mapredue, обычно являются дисковыми приложениями，
 Spark обычно является ресурсоемким приложением, а холодные архивы данных обычно требуют хранения с высокой плотностью.Базовые аналитические кластеры со смешанной нагрузкой должны иметь возможность иерархического хранения и планирования этикеток.
``
 
### Сетевая плоскость
``
 Вся системная сеть разделена на две плоскости: бизнес-плоскость и план управления, развернутая с физической изоляцией между двумя плоскостями для обеспечения безопасности бизнеса и управления соответствующей сетью.
 Бизнес-плоскость обеспечивает доступ к данным, отправку задач и вычислительную мощность через доступ к бизнес-сети, в основном для пользователей и пользователей верхнего уровня.
 Плоскость управления обеспечивает управление системой и функции обслуживания через оперативный и эксплуатационный доступ к сети, в основном используется для управления кластером, внешнего мониторинга кластера, конфигурации, аудита, управления пользователями и других услуг.
``
 
### Типичная настройка бизнеса
``
 Бизнес компонентов верхнего уровня обычно можно разделить на интенсивные операции ввода-вывода, интенсивные вычислительные операции, услуги с низкой задержкой и услуги с высокой пропускной способностью.
 
 [1] низкий бизнес задержки
 
 Вычислительно-интенсивный бизнес, бизнес с низкой задержкой, этот тип бизнеса, как правило, ориентирован на доступ к большому количеству каталогов файлов NameNode.
 Поэтому необходимо настроить вычислительную мощность для NameNode.Основные меры могут быть приняты：
 1.Используйте federatian для добавления новых реализаций схемы NameNode.Разверните высокоприоритетный бизнес в отдельном NameNode для решения проблемы использования ресурсов.
 2.В периоды пикового трафика можно временно отключить журнал аудита HDFS и изменить уровень журнала выполнения на уровень WARN.Когда бизнес снижается, он может быть настроен на информационный уровень.
 3.Используйте функцию изоляции ввода-вывода DataNode HDFS, чтобы увеличить значение веса для трафика с низкой задержкой.
 4.Повышение производительности оборудования (повышение тактовой частоты ЦП узла NameNode,замена диска DataNode на высокопроизводительный диск или SSD,уменьшение задержки пропускной способности сети.).
 
 [2] высокий объем дела
 Высокая пропускная способность, эта категория бизнеса в основном связана с большими файловыми операциями,которые требуют более высокого ввода-вывода для DataNode.Основные меры могут быть приняты：
 1.Увеличьте количество узлов DataNode и увеличьте пропускную способность кластера.
 2.Увеличьте количество дисков для узлов DataNode, чтобы увеличить возможности ввода-вывода для одного DataNode.
 3.Замените тип диска, на котором установлен DataNode, на более производительный диск или SSD.
 
 [3] данные в реальном времени
 Данные в реальном времени требуют, чтобы HDFS быстро отвечала на бизнес-запросы,предъявляя высокие требования к производительности узла.Для обеспечения реального времени бизнеса,
 Обычно используется сценарий, который настраивает высокопроизводительные узлы и изолирует бизнес.Основные меры могут быть приняты:
 
 1.Используя хранилище тегов, узлы хранения сгруппированы по производительности, что позволяет высокопроизводительным узлам обслуживать их.
 2.Узел данных использует носитель SSD.
 3.Используйте атрибут Федерации, чтобы изолировать NameNode, а также узел данных.
 
 [4] Частые данные доступа
 Часто используемые данные обычно относятся к данным, которые требуют повторного чтения после завершения записи, а иногда и изменения.
 
 Для этой категории данных могут быть приняты следующие меры：
 1.Используйте атрибут Central Cache, чтобы кэшировать соответствующие файлы в памяти.
 2.Сохраните соответствующие файлы на носителе SSD, используя характеристики иерархического хранилища.
 
 [5] временные данные 
 Временные данные обычно относятся к бизнес-обработке сразу после завершения записи и удалению данных после завершения обработки.
 Для этой категории данных могут быть приняты следующие меры：
 1.Используйте иерархическое хранилище для хранения данных на RAM-диске.
 2.Установите копии на 2, уменьшите нагрузку на системное хранилище и сеть.
 
 [6] архивирование данных
 Архивные данные-это данные, которые больше не доступны через некоторое время после хранения данных в кластере：
 Для этой категории данных могут быть приняты следующие меры：
 
 1.Возможность управления жизненным циклом данных HDFS (горячая и холодная миграция данных)，
 Холодные данные можно переносить в архивный каталог или на носитель с более низкой производительностью ввода-вывода, уменьшать количество копий и т. д.
 2.При хранении данных они могут храниться в формате HRM или использовать функцию слияния небольших файлов SmallFS.
 3. Храните эти данные на узлах с высокой плотностью (архив), используя функцию хранения тегов. 
``
 
### Сеть кластеров
 - Трехслойная сеть ECMP
 
 ![Кластерный сетевой узел] (img / network.джпг)
 
 - Групповые сетевые точки：
 
``
 Сплющивание 3 слоев плоское
 ECMP позволяет агрегировать распределение нагрузки
 Сходящийся маршрут может быть линейно расширен
 Одна логическая стойка одна подсеть, два уровня коммутации в стойке
``
 
 - Применимые сцены:
 
``
 Число узлов кластера более 500 или более поздних может быть увеличено до более 500
 Коммутатор доступа / конвергенции поддерживает трехуровневую коммутацию 
``
 
### Конфигурация сетевой карты хоста
``
 Каждый узел оснащен двумя портами Бонда, соответственно доступ к двум коммутаторам доступа не складывается без Бонда TRUNK Mode: mode = 6 
 Используется стратегия адаптивной нагрузки (баланс-alb-rlb,адаптивная балансировка нагрузки).
 Сетевой трафик bond распределяется поровну на двух субпортах с большим конвергентным трафиком
``
### планирование главного узла
``
 Роли, связанные с основным узлом, помещаются в разные стойки, узлы Zookeeper, JournalNode помещаются в разные стойки
 nodemanager обычно смешивается только с datanode, datanode может смешиваться с nodemanager/regionserver/Kafka brocker/storm supervisor/solr/es / presto worker и т. д.
 
 master0 (JournalNode / zk / HMaster / HiveServer / MetaStore / WebHCat / spark JobHistory / ThriftServer)
 master1 (zk / HiveServer / MetaStore / WebHCat / spark JobHistory / ThriftServer)
 master2 (zk / HMaster / HiveServer / MetaStore / WebHCat / ThriftServer / EsMaster)
 master3 (zk / ResourceManager / EsMaster)
 master4 (zk / ResourceManager / EsMaster / mr JobHistoryServer)
 master5 (JournalNode/NameNode / Airflow master)
 master6 (журнал / NameNode / Airflow master)
 master7 (сервер ldap / keberos / Nimbus / UI)
 master8 (сервер ldap / keberos / Nimbus / UI)
 master9 (HiveServer / MetaStore / WebHCat / spark JobHistory / ThriftServer)
 master10 (HiveServer / MetaStore / WebHCat / spark JobHistory / ThriftServer)

``
 
 
## как насчет того, что # namenode является standy / active
``
 Остановка ZKFC
 
 Су-хдфс
 hdfs zkfc-formatZK
 
 рестарт ЗКФК & ХДФС

``

### Шаги по установке
``
 1 подготовка базы данных и среды java
 2 ссылка на выполнение https://raw.githubusercontent.com/xiaomatech/dataops/master/ambari_server_install.sh
 3 Обратите внимание на изменение конфигурации http://assets.example.com
 4 Подготовьте ресурсы к справочному каталогу веб-служб для ресурсов.джпг
``

### Загрузка каталога сервера
![каталог служб активов] (img / assets.джпг)


#### Установка компонентов перед развертыванием
``
### Требования к среде развертывания
1 имя хоста может быть разрешено через dns
 Метод проверки dig ' имя_хоста`
2 / etc / yum.РЕПО.d под yum источник нормальный
 Убедитесь,что jdk,ambari-agent,openldap-client, kerberos client установлены правильно
 yum install-y snappy snappy-devel lzo zlib bzip2 libzip openssl libtirpc-Devel tar 
 curl jdk openldap-clients nss-pam-ldapd pam_ldap pam_krb5 authconfig krb5-libs libcgroup java-1.8.0-openjdk
3 ambari-сервер имеет ключ rsa для входа в управляемую машину
 2 варианта:
 1, ключи pub, сгенерированные ambari-сервером, распределяются между управляемыми машинами
 2, Используйте локальную учетную запись, инициализированную при установке системы, аналогичную учетной записи Бога, такой как yunwei
4 управляемая машина должна иметь возможность пинговать через ambari-сервер, потому что для регистрации агента и отправки обратно сердцебиение
5 выполнение build_private_repo.sh/download_bigdata_plugin.sh/download_intel_mkl.sh 
 Загрузите компоненты локально, чтобы создать похожие зеркала.алиюн.серверы загрузки com
6 может пинговать через cmdb(необходимо получить данные Hadoop Rack-aware, если нет этого интерфейса, нет автоматической функции Rack-aware) 
``

### Порядок установки компонентов
``
1 Установка ЗК
2 Установите openldap / kdc или zookeeper-env и заполните соответствующие kdc_hosts, ldap_hosts
3 Запуск kerberos (необязательно)
``

### Автоматическое развертывание и масштабирование компонентов больших данных
``
введение блуэпринт
 возможности ambari для автоматического развертывания компонентов больших данных
сослаться
 https://cwiki.apache.org/confluence/display/AMBARI/Blueprints
 https://community.hortonworks.com/articles/78969/automate-hdp-installation-using-ambari-blueprints-4.html
``

### Шаги по управлению и расширению журнала компонентов
``
Введение в управление журналами
    Большие данные суммируются в hdfs в дополнение к собственным журналам заданий компонента
    Также в infra solr(solrcloud) хранятся журналы аудита ranger,функция поиска atlas,журнал выполнения компонентов(logsearch
Отношение соответствующих коллекций
    ranger - > ranger_audits сохраняет 14 дней по умолчанию
    logsearch - > hadoop_logs(журнал выполнения компонента), audit_logs (журнал аудита hdfs) по умолчанию сохраняется 7 дней
    атлас - > edge_index, vertex_index,fulltext_index по умолчанию сохраняется 7 дней
На этом этапе мы разделили 32 shard с solrcloud.

Расширенная пошаговая ссылка на официальную документацию solr
 ``
 
### расширение hdfs
``
Добавление машины и установка datanode в ambari
На странице ambari обновите конфигурацию под namenode(сделал Rack-aware не обновлен ,когда это default/default-rack, DataNode Plus не входит в кластер)
``
 
### namenode/ldap/kdc/keytab резервное копирование и восстановление
 ``
резервное копирование и восстановление метаданных namenode(fsimage)
    Резервная копия находится в/data / backup / namenode.Цикл резервного копирования: резервное копирование всех резервных копий каждый день в час ночи, 3 дня.
    备份命令/data/backup/namenode.sh
    Шаги по восстановлению : 
        https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.2/bk_hdfs-administration/content/get_ready_to_backup.html
резервное копирование и восстановление ldap/kdc
    резервная копия:
        На каждом kdc/data / backup / ldap 备份命令/data/backup/ldap.sh период резервного копирования: 3 дня в час
    восстановление:
        Сначала удалите все записи: ldapdelete-x-D "cn = manager, dc=example, dc = com "- w "password" - r " dc=example, dc = com"
        Восстановление: ldapadd-x-D "cn=manager, dc=example, dc = com" - F/data/backup/ldap / ldap_ Time.пароль ldif-w

резервное копирование и восстановление keytab
    Резервное копирование/etc/security/keytabs в/data/backup/keytab перед каждой операцией
    Поддержка возврата к предыдущему keytab
    При восстановлении следует помнить о сохранении прав доступа к файлам с помощью cp-rpf /data/backup / keytab / Time / keytabs/ * / etc / security / keytabs/
 ``

### Компоненты безопасности ranger Principia / Примечания / Плагины включены
``
Решенные проблемы
    унифицированный контроль разрешений для хдфс,Ярн,улей,искры,хбасе,шторм,Кафка,атлас,солр, ЕТК
Управляемые потоки данных
    ranger-admin отвечает за хранение политик и предоставляет http restful api для внешних служб запросов и изменений политик
    ranger-usersync отвечает за синхронизацию данных пользователей,групп пользователей из ldap в ranger
    ranger-tagsync отвечает за синхронизацию тегов на атласе с ranger
Поток данных плагина
    1 после включения плагина http периодически (по умолчанию 30 секунд)загружает политику из ranger-admin в локальный/etc / ranger
    2 путем разбора политики на что-то вроде большого hashmap, когда запрос приходит, есть filter(checkPermission) для управления разрешениями
    3 логи результатов записываются обратно в solr, hdfs(новая версия поддержки kafka не включена)
Компоненты, которые влияют
    В основном соответствующий главный узел: hdfs-namenode, yarn-resourcesmanager,hive-hiveserver2,
    hbase-мастер, regionserver,kafka-брокер,storm-Нимбус
быть внимательнее
    1 hadoop-env.sh и ... yarn-env.sh и ... hive-env.sh добавление jar ranger в соответствующий путь к классам позволяет найти соответствующий класс
    2 hdfs-сайт.xml, Ярн-сайт.xml, hbase-сайт.xml,hiveserver2.XML для присоединения к соответствующему классу filter

При установке в ranger-env вы можете изменить соответствующую версию, после изменения перезапуска соответствующего плагина будет автоматически обновлен до указанной версии плагина

``

### kerberos включен / проверка JCE keytab отсутствует что делать
``
представлять
    kerberos-это протокол безопасного взаимодействия
    jce-это пакет jar, используемый в jdk для шифрования
    в oracle jdk нет JCE, который необходимо установить самостоятельно
    openjdk поставляется с jce
Просмотр адреса kdc
    cat / etc / krb5.conf / grep ' kdc ='
Просмотр состояния jce в режиме kerberos должен быть включен
    /usr / java / default / bin / java-jar / var / lib / ambari-agent / tools / jcepolicyinfo.jar-tu

резервное копирование и восстановление keytab
    Резервное копирование/etc/security/keytabs в/data/backup/keytab перед каждой операцией
    Поддержка возврата к предыдущему keytab
    При восстановлении следует помнить о сохранении прав доступа к файлам с помощью cp-rpf /data/backup / keytab / Time / keytabs/ * / etc / security / keytabs/
 
обработка пропусков keytab
    Старайтесь не перестраивать
``
### Обновление компонентов
``
В дополнение к hdfs, поддержка перезагрузки после изменения конфигурации для автоматического обновления
    1 измените download_url соответствующего компонента
    2 Перезагрузка
    3 обновление завершено
``

### шаги по разработке # ambari_service
``
   1 понимание состава соответствующей службы, основной принцип состоит в том, что компонент управляет классом демонов, сколько классов демонов имеет сколько компонентов
   2 скопируйте каталог sample services\EXAMPLE
   3 модификация metainfo.имя в xml ,конфигурация компонента должна быть глобально уникальной 
   4 измените конфигурацию kerberos для kerberos.json
   5 изменение оповещений конфигурации монитора.json (может быть настроен с функцией мониторинга порта, мониторинга url)
   6 изменение конфигурации в каталоге конфигурации (это в основном используется для управления конфигурацией на веб-странице)
   7 реализация каталога пакетов реализация методов install, start, stop, status, restart (этот каталог используется для выполнения следующих задач агента)
   8 скопируйте соответствующий каталог в/var/lib/ambari-server/resources / stacks / XIAOMATECH / 1.0 / services
   9 перезагрузите ambari-сервер / etc / init.D / ambari-восстановление сервера
   10 перейдите по адресу <url>:8080 установите соответствующий сервис для развертывания
``

#### Справочник по большим данным
``
    1 наиболее авторитетными являются официальные документы
    2 официальная документация hdp https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.1.0/index.html
    3 реальное хранилище данных электронной коммерции детальное развитие всего процесса https://chuanke.baidu.com/v1538386-116215-258987.html 
    4 qingyun семь дней обучения Big Data Pro Edition https://study.163.com/course/courseMain.htm?courseId=1003605105
    5 основные технологии платформы больших данных Али http://www.xuetangx.com/courses/course-v1:TsinghuaX+60240202X + sp / о
    6 серия лекций "наука и приложения больших данных" http://www.xuetangx.com/courses/course-v1:TsinghuaX+60250131X + sp / о
``
### Компонентный сигнал тревоги монитора

``
Поставляется с поддержкой 5 типов сигналов мониторинга
    1 сигнал тревоги монитора порта для того чтобы контролировать ли один порт на машине доступен
    2 сигнал тревоги монитора кода состояния сети использован для того чтобы контролировать ли адрес ИП сети (URL) доступен
    3 логика мониторинга оповещения скрипта выполняется пользовательским скриптом python
    4 metric используется для мониторинга свойств конфигурации, связанных с Metric
    5 aggregate используется для сбора некоторых других состояний Alert
    
    Обратитесь к оповещениям в службе hdfs.конфигурация json
``
